{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sukai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk #natural lang toolkit\n",
    "nltk.download('punkt') #Punkt Sentence Tokenizer. This tokenizer divides a text into a list of sentences, by using an unsupervised algorithm\n",
    "import re #RegEx Module. Python has a built-in package called re , which can be used to work with Regular Expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tennis_articles_v4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maria Sharapova has basically no friends as te...</td>\n",
       "      <td>https://www.tennisworldusa.org/tennis/news/Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>BASEL, Switzerland (AP), Roger Federer advance...</td>\n",
       "      <td>http://www.tennis.com/pro-game/2018/10/copil-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Roger Federer has revealed that organisers of ...</td>\n",
       "      <td>https://scroll.in/field/899938/tennis-roger-fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Kei Nishikori will try to end his long losing ...</td>\n",
       "      <td>http://www.tennis.com/pro-game/2018/10/nishiko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Federer, 37, first broke through on tour over ...</td>\n",
       "      <td>https://www.express.co.uk/sport/tennis/1036101...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                       article_text  \\\n",
       "0           1  Maria Sharapova has basically no friends as te...   \n",
       "1           2  BASEL, Switzerland (AP), Roger Federer advance...   \n",
       "2           3  Roger Federer has revealed that organisers of ...   \n",
       "3           4  Kei Nishikori will try to end his long losing ...   \n",
       "4           5  Federer, 37, first broke through on tour over ...   \n",
       "\n",
       "                                              source  \n",
       "0  https://www.tennisworldusa.org/tennis/news/Mar...  \n",
       "1  http://www.tennis.com/pro-game/2018/10/copil-s...  \n",
       "2  https://scroll.in/field/899938/tennis-roger-fe...  \n",
       "3  http://www.tennis.com/pro-game/2018/10/nishiko...  \n",
       "4  https://www.express.co.uk/sport/tennis/1036101...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() #show 5 starting rows of csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Federer, 37, first broke through on tour over two decades ago and he has since gone on to enjoy a glittering career. The 20-time Grand Slam winner is chasing his 99th ATP title at the Swiss Indoors this week and he faces Jan-Lennard Struff in the second round on Thursday (6pm BST). Davenport enjoyed most of her success in the late 1990s and her third and final major tournament win came at the 2000 Australian Open. But she claims the mentality of professional tennis players slowly began to change after the new millennium. \"It seems pretty friendly right now,\" said Davenport. \"I think there is a really nice environment and a great atmosphere, especially between some of the veteran players helping some of the younger players out. \"It\\'s a very pleasant atmosphere, I\\'d have to say, around the locker rooms. \"I felt like the best weeks that I had to get to know players when I was playing were the Fed Cup weeks or the Olympic weeks, not necessarily during the tournaments. \"And even though maybe we had smaller teams, I still think we kept to ourselves quite a bit. \"Not always, but I really feel like in the mid-2000 years there was a huge shift of the attitudes of the top players and being more friendly and being more giving, and a lot of that had to do with players like Roger coming up. \"I just felt like it really kind of changed where people were a little bit, definitely in the 90s, a lot more quiet, into themselves, and then it started to become better.\" Meanwhile, Federer is hoping he can improve his service game as he hunts his ninth Swiss Indoors title this week. \"I didn\\'t serve very well [against first-round opponent Filip Kranjovic,\" Federer said. \"I think I was misfiring the corners, I was not hitting the lines enough. \"Clearly you make your life more difficult, but still I was up 6-2, 3-1, break points, so things could have ended very quickly today, even though I didn\\'t have the best serve percentage stats. \"But maybe that\\'s exactly what caught up to me eventually. It\\'s just getting used to it. This is where the first rounds can be tricky.\"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['article_text'][4] #showing article no 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#breaking text into individual sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = []\n",
    "for s in df['article_text']:\n",
    "    sentences.append(sent_tokenize(s))\n",
    "    \n",
    "sentences = [y for x in sentences for y in x] #flatten list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Maria Sharapova has basically no friends as tennis players on the WTA Tour.',\n",
       " \"The Russian player has no problems in openly speaking about it and in a recent interview she said: 'I don't really hide any feelings too much.\",\n",
       " 'I think everyone knows this is my job here.',\n",
       " \"When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match.\",\n",
       " \"I'm a pretty competitive girl.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    #print(values[0])\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    #print(coefs)\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_embeddings.keys()\n",
    "#values[0]\n",
    "#len(word_embeddings)\n",
    "#word_embeddings['small'] #We now have word vectors for 400,000 different terms stored in the dictionary – ‘word_embeddings’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing/Cleaning\n",
    "#remove punctuations, numbers and special characters\n",
    "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when i m on the courts or when i m on the court playing  i m a competitor and i want to beat every single person whether they re in the locker room or across the net so i m not the one to strike up a conversation about the weather and know that in the next few minutes i have to go and try to win a tennis match ',\n",
       " 'i m a pretty competitive girl ',\n",
       " 'i say my hellos  but i m not sending any players flowers as well ',\n",
       " 'uhm  i m not really friendly or close to many players ',\n",
       " 'i have not a lot of friends away from the courts  ',\n",
       " 'when she said she is not really close to a lot of players  is that something strategic that she is doing ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentences[3:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' no  not at all ',\n",
       " 'i think just because you re in the same sport doesn t mean that you have to be friends with everyone just because you re categorized  you re a tennis player  so you re going to get along with tennis players ',\n",
       " 'i think every person has different interests ',\n",
       " 'i have friends that have completely different jobs and interests  and i ve met them in very different parts of my life ',\n",
       " 'i think everyone just thinks because we re tennis players we should be the greatest of friends ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentences[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sukai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') #Get rid of the stopwords (commonly used words of a language – is, am, the, of, in, etc.) present in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "stop_words[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " #define a function to remove these stopwords from our dataset\n",
    "# function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['second seeded anderson defeated fernando verdasco',\n",
       " 'anderson shot fifth career title second year winning new york february',\n",
       " 'nishikori leads anderson career matchups south african previous meeting year',\n",
       " 'victory sunday anderson qualify atp finals',\n",
       " 'currently ninth place nishikori win could move within points cut eight man event london next month',\n",
       " 'nishikori held serve throughout kukushkin came qualifying',\n",
       " 'used first break point close first set going second wrapping win first match point',\n",
       " 'verdasco anderson hit nine aces opening set',\n",
       " 'spaniard broke anderson twice second get another chance south african serve final set',\n",
       " 'federer first broke tour two decades ago since gone enjoy glittering career']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentences[50:60] #clean from punctuations and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector Representation of Sentences\n",
    "#use clean_sentences to create vectors for sentences in our data with the help of the GloVe word vectors\n",
    "\n",
    "#Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word_embeddings is 400000 words\n",
      "For Example:\n",
      "The word \"maria\" has 1D array of 100 values: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.089831 , -0.48885  , -0.43325  ,  0.40843  , -0.31087  ,\n",
       "       -0.028368 ,  0.23481  ,  0.59285  ,  0.14395  , -0.031488 ,\n",
       "        0.59812  ,  0.60423  , -1.0204   ,  0.94414  , -0.20685  ,\n",
       "       -0.71604  ,  0.35925  ,  0.54906  , -0.81433  ,  0.85464  ,\n",
       "       -0.77     ,  0.17722  ,  0.46829  ,  2.1848   , -0.30356  ,\n",
       "       -0.14618  , -0.031721 , -1.237    ,  0.99722  ,  0.6067   ,\n",
       "       -0.14535  , -0.31622  ,  1.0659   ,  0.17084  ,  0.572    ,\n",
       "       -0.26004  ,  0.28351  , -0.05624  ,  0.51423  ,  0.24415  ,\n",
       "       -0.22479  , -0.42128  ,  0.5311   ,  0.01898  ,  0.51962  ,\n",
       "       -0.95861  , -0.29593  , -0.59059  ,  0.11893  ,  1.4661   ,\n",
       "       -0.037916 ,  0.32234  ,  0.271    ,  0.39239  , -0.12836  ,\n",
       "       -2.3792   , -0.67754  ,  0.79619  , -0.33012  ,  0.15114  ,\n",
       "        0.09709  ,  0.56912  ,  0.078851 ,  0.0039787, -0.53211  ,\n",
       "       -0.20123  ,  0.29722  ,  0.44458  ,  0.65177  , -0.23854  ,\n",
       "        0.48704  ,  0.40112  ,  0.007153 , -0.38124  ,  0.43798  ,\n",
       "        0.1751   , -0.2575   ,  0.13136  , -0.19392  , -0.14044  ,\n",
       "       -0.15252  , -0.50809  , -0.12516  ,  0.18866  , -0.93285  ,\n",
       "       -1.142    , -0.80889  ,  0.49331  ,  0.023882 , -1.3505   ,\n",
       "        0.25478  , -0.60499  ,  0.78126  ,  0.74593  , -0.44008  ,\n",
       "        0.082123 , -0.23933  , -1.003    , -0.16146  , -0.03224  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('length of word_embeddings is',len(word_embeddings),'words')\n",
    "print('For Example:')\n",
    "print('The word \"maria\" has 1D array of 100 values: ')\n",
    "word_embeddings['maria']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#create vectors for our sentences. \n",
    "#total 119 sentences\n",
    "sentence_vectors = []\n",
    "#n=1\n",
    "for i in clean_sentences:\n",
    "    #print(\"i is:\")\n",
    "    #print(n)\n",
    "    #n=n+1\n",
    "    #print(i)\n",
    "    #print(len(i))\n",
    "    if len(i) != 0:\n",
    "        #if(n==9):\n",
    "            #print(i.split())\n",
    "            #print(len(i.split()))\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "            #print(v)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vectors for our sentences. \n",
    "#total 119 sentences\n",
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of \"sentence_vectors\" is 119\n",
      "At every index of \"sentence_vectors\" there are total 100 elements\n",
      "For Example:\n",
      "For sentence \" lot friends away courts \" , The vector will be: \n",
      "[ 0.24104375  0.12306623  0.32634842 -0.16956511 -0.224022    0.66671085\n",
      " -0.7403074   0.21959734 -0.06707998 -0.41248187  0.25100324 -0.00329018\n",
      "  0.15550862  0.02710397 -0.23776732 -0.370979   -0.13901925  0.31817797\n",
      " -0.55909026  0.5518795   0.25950262  0.57637095  0.39584956 -0.41048217\n",
      " -0.06168208 -0.04556735 -0.16670083 -0.70130223  0.21934514  0.03684579\n",
      "  0.25546113  0.25386906  0.1603599  -0.00866033  0.12825543  0.24463636\n",
      " -0.26621345  0.11710472 -0.06700075 -0.0758443  -0.35009795 -0.40068984\n",
      "  0.11256685 -0.65367657 -0.2524094  -0.32909423 -0.04898526  0.1811647\n",
      "  0.32104275 -0.5225219  -0.03068833 -0.41489878  0.09094851  0.99126726\n",
      "  0.15736736 -1.9094627   0.02594076 -0.11118271  1.4299649   0.45726067\n",
      " -0.19966184  1.0680655  -0.11626158  0.10241841  0.5532142  -0.01284931\n",
      "  0.40289178  0.21113348 -0.04540089  0.01537491 -0.11402399 -0.16454387\n",
      "  0.25130844 -0.5405846   0.06922444  0.22715543  0.09173182 -0.20500581\n",
      " -0.37396753  0.21804725  0.607788    0.03681255 -0.26776055 -0.06167054\n",
      " -1.2994251  -0.03100199 -0.37518123  0.06647588 -0.42091727 -0.3352482\n",
      " -0.11996276 -0.3282879   0.03865957 -0.0104024  -0.39140216 -0.11795551\n",
      " -0.20538355  0.25661734  0.64003     0.4415696 ]\n"
     ]
    }
   ],
   "source": [
    "print('Length of \"sentence_vectors\" is',len(sentence_vectors)) #119 sentences\n",
    "print('At every index of \"sentence_vectors\" there are total',len(sentence_vectors[7]),'elements') #at every index of this list, 100 values are residing\n",
    "print('For Example:')\n",
    "print('For sentence \"',clean_sentences[7],'\" , The vector will be: ') \n",
    "print(sentence_vectors[7]) #100 i.e each index of this 1D list array/list/vector has 100 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14161"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Similarity Matrix Preparation\n",
    "#to find similarities between the sentences, we use the cosine similarity approach for this challenge\n",
    "#create an empty similarity matrix for this task and populate it with cosine similarities of the sentences\n",
    "\n",
    "#first define a zero matrix of dimensions (n * n).  We will initialize this matrix with cosine similarity scores of the sentences. Here, n is the number of sentences.\n",
    "\n",
    "#similarity matrix\n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "sim_mat.shape #119 rows, 119 col in each row\n",
    "sim_mat.size #14161 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#initialize the matrix with cosine similarity scores\n",
    "#n=0\n",
    "for i in range(len(sentences)):\n",
    "  for j in range(len(sentences)):\n",
    "    if i != j:\n",
    "        #print('i is')\n",
    "        #print(len(i))\n",
    "        #print((sentence_vectors[1]))\n",
    "        #print(len(sentence_vectors[1]))\n",
    "        #if(n==7):\n",
    "            #print(clean_sentences[n])\n",
    "        sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "        #if(sim_mat[7][n]>0.5):\n",
    "            #print(clean_sentences[n])\n",
    "        #n=n+1\n",
    "        #print(sim_mat[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence no: 3\n",
      "i.e:'courts court playing competitor want beat every single person whether locker room across net one strike conversation weather know next minutes go try win tennis match '\n",
      "has following vector representation:\n",
      "[[ 0.00635764  0.19392036  0.47338355 -0.11994532 -0.18183222  0.39362404\n",
      "   0.03021473  0.3058678  -0.27984452 -0.1350603   0.21782461 -0.0435336\n",
      "   0.05983898 -0.03847076 -0.03452814 -0.08754686 -0.03284152  0.26497212\n",
      "  -0.40574118  0.2045678   0.30722964  0.14276573  0.2527146  -0.03963274\n",
      "   0.23167998 -0.11476567 -0.05163356 -0.46983826  0.24158174 -0.04750975\n",
      "  -0.21098635  0.42772007  0.13107622 -0.0298439   0.17862822  0.13905041\n",
      "  -0.4005354   0.20550247  0.01217696 -0.19218549 -0.15770629 -0.29531738\n",
      "   0.13991627 -0.39578143 -0.02201634 -0.03285473  0.07069061 -0.31716087\n",
      "   0.0904976  -0.79340553  0.02686624 -0.07301015  0.0272568   0.9243445\n",
      "   0.02536825 -2.168561   -0.12019134  0.06289279  1.432541    0.5427988\n",
      "  -0.2916361   0.5324056  -0.27415586  0.04131274  0.42412865  0.07015922\n",
      "   0.25089505  0.321241   -0.2598141  -0.10732103 -0.02452752 -0.18340324\n",
      "   0.12241655 -0.32825926  0.19089122  0.13729095 -0.05409885 -0.3381081\n",
      "  -0.29546994  0.12312843  0.54616    -0.13443445 -0.38527927 -0.04051573\n",
      "  -1.2127631  -0.35605088 -0.10221499  0.21508756 -0.04328202 -0.1884288\n",
      "  -0.02893854 -0.00797384 -0.05258244 -0.02768826 -0.33510622  0.11183578\n",
      "   0.00646364  0.08232711  0.46385223  0.15356502]]\n",
      "[[-1.51949348e-02  2.30953023e-01  2.75334895e-01 -2.62705773e-01\n",
      "  -4.24148649e-01  6.42053261e-02 -1.62395556e-02  2.68650472e-01\n",
      "  -1.36607796e-01 -3.54385264e-02 -1.06021296e-02 -4.96394597e-02\n",
      "   2.51724780e-01 -1.13888390e-01 -1.00029975e-01  6.20943084e-02\n",
      "  -8.92735794e-02  2.49413550e-01  1.19463518e-01  5.56387544e-01\n",
      "   3.78456444e-01  3.31889361e-01  2.17869222e-01 -7.52349123e-02\n",
      "   1.51103631e-01  3.00866365e-01 -4.33572143e-01 -9.66094673e-01\n",
      "  -3.22509170e-01  3.96415532e-01 -1.21167257e-01  5.98753750e-01\n",
      "   5.07392466e-01 -5.52219272e-01  7.22825706e-01 -2.68313229e-01\n",
      "  -6.68357313e-01 -8.05668086e-02  1.29633471e-01 -4.00726438e-01\n",
      "   1.06351212e-01 -1.96084585e-02 -1.25231575e-02 -2.22553506e-01\n",
      "  -1.68975681e-01  2.86204591e-02  8.96367878e-02  3.28277260e-01\n",
      "   2.66879052e-01 -8.83092284e-01 -3.01920682e-01 -4.87037659e-01\n",
      "   4.46884409e-02  7.58327186e-01  2.27626786e-01 -2.03328896e+00\n",
      "   1.50589794e-01  2.01816067e-01  1.12418187e+00  1.32406533e-01\n",
      "   9.49004292e-02  6.92185938e-01 -8.72852445e-01  2.90975273e-02\n",
      "   4.72625792e-01 -2.23325603e-02  5.74876368e-01  3.41052935e-02\n",
      "   8.76811147e-02  3.09743434e-02  2.67667443e-01 -2.49046758e-01\n",
      "   4.11855988e-02  3.12172621e-01  3.03842068e-01  6.32475913e-01\n",
      "   2.07114331e-02 -2.77397513e-01 -3.88419390e-01 -5.33198953e-01\n",
      "   2.00566471e-01  2.65443861e-01 -5.56914359e-02  1.91303249e-02\n",
      "  -1.10769081e+00 -7.61482894e-01 -3.30833077e-01 -7.29759689e-04\n",
      "  -3.80017370e-01 -5.08913696e-01 -2.51932703e-02  2.36316234e-01\n",
      "  -1.31856063e-02 -1.00980140e-01 -2.20724091e-01  2.19783083e-01\n",
      "  -5.90413570e-01 -1.56804413e-01  1.77140936e-01  3.46361220e-01]]\n"
     ]
    }
   ],
   "source": [
    "#initialize the matrix with cosine similarity scores\n",
    "for i in range(len(sentences)):\n",
    "    if(i==3):\n",
    "        print(\"The sentence no:\",i)\n",
    "        print(\"i.e:'\"+clean_sentences[i],\"'\")\n",
    "        print(\"has following vector representation:\")\n",
    "        print(sentence_vectors[i].reshape(1,100))\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            if(i==3 and j==4):\n",
    "                print(sentence_vectors[j].reshape(1,100))\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.855755\n",
      "4.3120008\n",
      "12.712149\n",
      "0.7645936932788024\n"
     ]
    }
   ],
   "source": [
    " #print(sentence_vectors[3]*sentence_vectors[4])\n",
    "print(np.linalg.norm(sentence_vectors[3]))\n",
    "print(np.linalg.norm(sentence_vectors[4]))\n",
    "print(np.dot(sentence_vectors[3],sentence_vectors[4]))\n",
    "ansr = 12.712149 / (3.855755*4.3120008)\n",
    "print(ansr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7645934820175171"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_mat[3][4] # showing sentence 3 cosine similarities with other 118 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lot friends away courts\n",
      "[ 0.24104375  0.12306623  0.32634842 -0.16956511 -0.224022    0.66671085\n",
      " -0.7403074   0.21959734 -0.06707998 -0.41248187  0.25100324 -0.00329018\n",
      "  0.15550862  0.02710397 -0.23776732 -0.370979   -0.13901925  0.31817797\n",
      " -0.55909026  0.5518795   0.25950262  0.57637095  0.39584956 -0.41048217\n",
      " -0.06168208 -0.04556735 -0.16670083 -0.70130223  0.21934514  0.03684579\n",
      "  0.25546113  0.25386906  0.1603599  -0.00866033  0.12825543  0.24463636\n",
      " -0.26621345  0.11710472 -0.06700075 -0.0758443  -0.35009795 -0.40068984\n",
      "  0.11256685 -0.65367657 -0.2524094  -0.32909423 -0.04898526  0.1811647\n",
      "  0.32104275 -0.5225219  -0.03068833 -0.41489878  0.09094851  0.99126726\n",
      "  0.15736736 -1.9094627   0.02594076 -0.11118271  1.4299649   0.45726067\n",
      " -0.19966184  1.0680655  -0.11626158  0.10241841  0.5532142  -0.01284931\n",
      "  0.40289178  0.21113348 -0.04540089  0.01537491 -0.11402399 -0.16454387\n",
      "  0.25130844 -0.5405846   0.06922444  0.22715543  0.09173182 -0.20500581\n",
      " -0.37396753  0.21804725  0.607788    0.03681255 -0.26776055 -0.06167054\n",
      " -1.2994251  -0.03100199 -0.37518123  0.06647588 -0.42091727 -0.3352482\n",
      " -0.11996276 -0.3282879   0.03865957 -0.0104024  -0.39140216 -0.11795551\n",
      " -0.20538355  0.25661734  0.64003     0.4415696 ]\n"
     ]
    }
   ],
   "source": [
    "print(clean_sentences[7]) \n",
    "print(sentence_vectors[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said really close lot players something strategic\n",
      "[ 2.42556762e-02  1.91585466e-01  4.32216644e-01 -2.52976745e-01\n",
      " -2.80925572e-01 -3.55560213e-01 -1.39371946e-01  2.86665920e-04\n",
      " -9.25063491e-02 -1.24433659e-01  2.21810028e-01 -4.02831621e-02\n",
      "  2.01679334e-01 -1.45522386e-01  3.73096652e-02 -3.24209958e-01\n",
      " -2.53245682e-01  1.83614790e-01 -4.84565049e-01  4.58421677e-01\n",
      "  3.01069856e-01  4.24084008e-01 -1.80378526e-01 -4.17533189e-01\n",
      " -8.28024372e-03 -4.15474959e-02 -8.52875412e-02 -4.98363048e-01\n",
      "  8.60114172e-02 -5.21492660e-02 -3.54141712e-01  5.94758093e-01\n",
      "  1.71603973e-03 -1.33746997e-01  3.34200829e-01  4.96243350e-02\n",
      " -4.68928739e-02  3.62201542e-01 -6.37499522e-03 -1.53245687e-01\n",
      " -2.13869035e-01 -2.26746172e-01  4.16741878e-01 -3.93372864e-01\n",
      " -4.32158291e-01 -5.07090427e-02  2.26316825e-01 -9.01343394e-03\n",
      " -2.42991582e-01 -9.66836572e-01  1.79266095e-01 -3.21795464e-01\n",
      " -1.93426654e-01  7.96500504e-01 -3.03493813e-02 -2.34997869e+00\n",
      "  7.27310404e-02 -8.77520442e-02  1.35656333e+00  6.05509222e-01\n",
      " -1.21027566e-01  8.88882935e-01 -4.38715905e-01  8.84516537e-02\n",
      "  4.54913020e-01 -1.21089846e-01  6.83541000e-01  4.37015593e-01\n",
      "  6.81128427e-02  1.86701864e-02  2.49134392e-01 -3.30364376e-01\n",
      "  1.55094847e-01 -5.94704151e-01  2.66943723e-01  4.91857110e-03\n",
      " -2.62249671e-02  4.95580658e-02 -7.03355193e-01  2.71118134e-01\n",
      "  6.87107146e-01 -2.40021460e-02 -5.20057559e-01 -1.19835563e-01\n",
      " -1.33529353e+00 -2.01675043e-01 -3.39957624e-04  5.21615893e-02\n",
      " -3.92523497e-01 -4.15630788e-01 -6.85689272e-03  7.24109337e-02\n",
      " -4.50448506e-02 -1.56480491e-01 -5.25884330e-01  2.18420371e-01\n",
      " -2.41560057e-01 -1.99890599e-01  1.97309539e-01  7.48844385e-01]\n"
     ]
    }
   ],
   "source": [
    "print(clean_sentences[8]) \n",
    "print(sentence_vectors[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "different men tour women tour\n",
      "[ 0.14655069  0.34374526  0.32507718 -0.07232994  0.08871686  0.35947415\n",
      "  0.28422055  0.7883484  -0.6572346   0.06513118  0.11985283 -0.07987602\n",
      "  0.504769    0.18137372 -0.17395921  0.03836792  0.24141972 -0.11861248\n",
      " -0.6466067   0.12568486  0.5576805   0.07137993  0.24266347  0.335113\n",
      " -0.03284143 -0.11493864  0.2869334  -0.8442571   0.68678665 -0.28883025\n",
      " -0.3020556   0.1460278  -0.19064291 -0.30805638  0.60939014  0.08740252\n",
      " -0.40993404  0.8347568  -0.55927235  0.21049112 -0.46041188 -0.04771065\n",
      "  0.31120455 -0.11008918 -0.00834194 -0.0360078  -0.06460588 -0.15042171\n",
      "  0.03572765 -0.79981005 -0.38245752 -0.15184364  0.02469586  0.8678783\n",
      "  0.08626554 -2.2610078  -0.1235815   0.25363147  1.2868828   0.7029874\n",
      " -0.37281346  0.875321    0.47034395 -0.06719477  0.5881744   0.12787843\n",
      "  0.05199499  0.26082143  0.28477466 -0.2895727   0.00311558 -0.20167366\n",
      " -0.39513096  0.04506318  0.0535933   0.47281504  0.33170766 -0.12626275\n",
      " -0.3347411  -0.61942613  0.5172925  -0.02304379  0.07188362  0.24251594\n",
      " -1.4620876  -0.2475945  -0.6076884  -0.26895723 -0.07321537  0.20821796\n",
      " -0.3780402  -0.04570886  0.4123615  -0.12977225 -0.1819376   0.00497101\n",
      " -1.0040032   0.11995062  0.6657209   0.36487505]\n"
     ]
    }
   ],
   "source": [
    "print(clean_sentences[9]) \n",
    "print(sentence_vectors[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74754524, 0.85698557, 0.82313859, 0.        , 0.76459348,\n",
       "       0.82157695, 0.86001396, 0.87651867, 0.88054085, 0.78653699,\n",
       "       0.        , 0.91478759, 0.86048663, 0.82472861, 0.86071944,\n",
       "       0.85267258, 0.77808994, 0.73368621, 0.78354514, 0.86032194,\n",
       "       0.91203374, 0.67856562, 0.74370331, 0.86674416, 0.80968148,\n",
       "       0.57839596, 0.80623901, 0.8662852 , 0.87715977, 0.88790208,\n",
       "       0.91792327, 0.90000653, 0.75233263, 0.9065733 , 0.85920221,\n",
       "       0.89490825, 0.69044781, 0.85199022, 0.93148559, 0.8759582 ,\n",
       "       0.78615373, 0.83042663, 0.72314888, 0.7770738 , 0.88384777,\n",
       "       0.85241956, 0.86992574, 0.63158852, 0.70109236, 0.82428443,\n",
       "       0.56804693, 0.83745837, 0.81676972, 0.74508065, 0.92414796,\n",
       "       0.7928772 , 0.92137158, 0.77083373, 0.91267812, 0.8636508 ,\n",
       "       0.80555663, 0.86975712, 0.87420946, 0.88090348, 0.87490869,\n",
       "       0.78270239, 0.93469763, 0.89488471, 0.9283427 , 0.87324065,\n",
       "       0.89103377, 0.85264019, 0.83447689, 0.92480159, 0.85427868,\n",
       "       0.81723487, 0.78381979, 0.80890334, 0.86685365, 0.85978985,\n",
       "       0.85933286, 0.81374466, 0.85507089, 0.86335403, 0.86151451,\n",
       "       0.77844179, 0.82932484, 0.84504747, 0.80531436, 0.51351511,\n",
       "       0.87634039, 0.85439515, 0.89474487, 0.85184491, 0.8205696 ,\n",
       "       0.9086352 , 0.7992661 , 0.89749724, 0.84424484, 0.71846062,\n",
       "       0.81673032, 0.85955536, 0.87686312, 0.90671003, 0.86458272,\n",
       "       0.92819202, 0.93104374, 0.86408103, 0.85322034, 0.77858895,\n",
       "       0.84066236, 0.81666529, 0.61817783, 0.81393236, 0.90837014,\n",
       "       0.90297091, 0.86781669, 0.82849979, 0.92567563])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### sim_mat[7].shape\n",
    "sim_mat[3] #showing that sentence 7 has similarity with sentence 8 and 9 \n",
    "#sim_mat[7][8]\n",
    "#sim_mat[7][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Applying PageRank Algorithm\n",
    "#convert the similarity matrix sim_mat into a graph\n",
    "import networkx as #print(\"Nodes of graph: \")\n",
    "#print(nx_graph.nodes())\n",
    "#print(\"Edges of graph: \")\n",
    "#print(nx_graph.edges())\n",
    "#nx.draw(nx_graph)\n",
    "#plt.savefig(\"proj.png\") # save as png\n",
    "#plt.show() # displaynx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat) # The nodes of this graph will represent the sentences and the edges will represent the similarity scores between the sentences\n",
    "\n",
    "scores = nx.pagerank(nx_graph) #apply the PageRank algorithm to arrive at the sentence rankings.\n",
    "nx.draw(nx_graph)\n",
    "plt.savefig(\"proj2.png\") # save as png\n",
    "plt.show() # display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Applying PageRank Algorithm\n",
    "#convert the similarity matrix sim_mat into a graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat) \n",
    "# The nodes of this graph will represent the sentences \n",
    "#the edges will represent the similarity scores between the sentences\n",
    "#print(len(nx_graph))\n",
    "#print(nx_graph.edges)\n",
    "#print(nx_graph.nodes)\n",
    "scores = nx.pagerank(nx_graph) #apply the PageRank algorithm to arrive at the sentence rankings.\n",
    "\n",
    "plt.savefig(\"AIproject.png\") # save as png\n",
    "plt.show() # display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nx.draw(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.47805712e+187 2.63715572e+187 2.38270958e+187 2.92110043e+187\n",
      " 2.27084864e+187 2.50659679e+187 2.62607825e+187 2.54041717e+187\n",
      " 2.66891588e+187 2.55053573e+187]\n",
      "{0: 0.008072651865276066, 1: 0.008501993259365037, 2: 0.0078119318398216, 3: 0.009293791260564146, 4: 0.007500319295916385, 5: 0.008146814785247897, 6: 0.008477413381565426, 7: 0.008251000819455925, 8: 0.008596957762357726, 9: 0.008257144250233068, 10: 0.0012695751770095795, 11: 0.008860552409260931, 12: 0.00808354331891815, 13: 0.008156804650453691, 14: 0.008443316877797879, 15: 0.008556893043719335, 16: 0.007812826653904838, 17: 0.008071958049751223, 18: 0.008406020961271342, 19: 0.0088478922486596, 20: 0.008860865186249187, 21: 0.007421917083656914, 22: 0.008223434004980818, 23: 0.008991766451813816, 24: 0.00846397039711992, 25: 0.006701898152599973, 26: 0.008232471647417278, 27: 0.008913135600535109, 28: 0.009061682997321345, 29: 0.009093905696447463, 30: 0.00924452161472398, 31: 0.008994323963323616, 32: 0.0072368691033659, 33: 0.008709093081740912, 34: 0.00891913055441074, 35: 0.009097421351821915, 36: 0.007715970774354713, 37: 0.00888345207561318, 38: 0.009260614592351134, 39: 0.009094807375780006, 40: 0.00803167375590389, 41: 0.007971530780677154, 42: 0.007049062090031209, 43: 0.007933352661687702, 44: 0.009059878155886734, 45: 0.008547315097131393, 46: 0.009014976104354611, 47: 0.007259944367042847, 48: 0.007911644621636307, 49: 0.00868151265253887, 50: 0.0066964345920045186, 51: 0.00870000260707955, 52: 0.008665996974112963, 53: 0.008118560923342462, 54: 0.00920358812889656, 55: 0.008276617499763795, 56: 0.009136178522364903, 57: 0.008011884120515293, 58: 0.009104756512964777, 59: 0.008943539656119303, 60: 0.008546416451353922, 61: 0.009089363733591293, 62: 0.00874902919395214, 63: 0.00865652815829764, 64: 0.00861730994447221, 65: 0.007276030255151521, 66: 0.009213900380764764, 67: 0.00867461704612636, 68: 0.009016694534669996, 69: 0.008436192849002313, 70: 0.00901656828206579, 71: 0.008817234006022297, 72: 0.007873247640152951, 73: 0.009031689976700357, 74: 0.008107483414425395, 75: 0.00787748700273138, 76: 0.008035132109809422, 77: 0.008574003227645715, 78: 0.008966528078812212, 79: 0.008912100676242814, 80: 0.008688526247788991, 81: 0.008622437805081328, 82: 0.00880365170755149, 83: 0.008945846994192833, 84: 0.008678156651014172, 85: 0.008306574974473098, 86: 0.008747703132864673, 87: 0.008731683812538225, 88: 0.008467251034212484, 89: 0.005651524783028771, 90: 0.00899682874735383, 91: 0.008552831102485624, 92: 0.008774774579595671, 93: 0.008831355573460856, 94: 0.008709578511773028, 95: 0.008894836009354075, 96: 0.00838868587036203, 97: 0.008856328238433952, 98: 0.008420356619710249, 99: 0.007851104189963317, 100: 0.008621842542731553, 101: 0.008835781696526176, 102: 0.008928322661634755, 103: 0.009040595529500841, 104: 0.008502881135170417, 105: 0.008880329984805492, 106: 0.00910262574670152, 107: 0.008609461622621625, 108: 0.008942212433660788, 109: 0.007757844377113239, 110: 0.008568300289671895, 111: 0.008082033127031726, 112: 0.006109321287044942, 113: 0.008327135508094772, 114: 0.008898758238907956, 115: 0.008715866238223277, 116: 0.008855870136476467, 117: 0.007889359600797438, 118: 0.009064760855823284}\n"
     ]
    }
   ],
   "source": [
    "pr_vector = np.array([1] * len(sim_mat))\n",
    "#len(pr_vector)\n",
    "damping = 0.85\n",
    "min_dif = 1e-5\n",
    "steps = 100\n",
    "previous_pr = 0\n",
    "for epoch in range(steps):\n",
    "    pr_vector = (1-damping)+damping*np.matmul(sim_mat,pr_vector)\n",
    "    if abs(previous_pr-sum(pr_vector))<min_dif:\n",
    "        break\n",
    "    else:\n",
    "        previous_pr=sum(pr_vector)\n",
    "        \n",
    "print(pr_vector[:10])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 3 has highest rank; 0.009293791260564146\n",
      "Sentence 10 has lowest rank; 0.0012695751770095795\n"
     ]
    }
   ],
   "source": [
    "#119  total scores\n",
    "max_score=max(scores.values()) #0.009293791260564146\n",
    "min_score=min(scores.values()) #0.0012695751770095795 \n",
    "for i,j in scores.items():\n",
    "    if (j == min(scores.values()) ):\n",
    "        print('Sentence',i,'has lowest rank;',min_score) # 3:0.009293791260564146 sentence 3 has highest ranking among all 119 sentences\n",
    "    if (j == max(scores.values()) ):\n",
    "        print('Sentence',i,'has highest rank;',max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(scores.values(),reverse=True) #sort values of scores dict in desc order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.009260614592351134,\n",
       " 'Major players feel that a big event in late November combined with one in January before the Australian Open will mean too much tennis and too little rest.')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summary Extraction)#Finally, it’s time to extract the top N sentences based on their rankings for summary generation.\n",
    "\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "ranked_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.009293791260564146, \"When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match.\")\n",
      "(0.009260614592351134, 'Major players feel that a big event in late November combined with one in January before the Australian Open will mean too much tennis and too little rest.')\n",
      "(0.00924452161472398, 'Speaking at the Swiss Indoors tournament where he will play in Sundays final against Romanian qualifier Marius Copil, the world number three said that given the impossibly short time frame to make a decision, he opted out of any commitment.')\n",
      "(0.009213900380764764, '\"I felt like the best weeks that I had to get to know players when I was playing were the Fed Cup weeks or the Olympic weeks, not necessarily during the tournaments.')\n",
      "(0.00920358812889656, 'Currently in ninth place, Nishikori with a win could move to within 125 points of the cut for the eight-man event in London next month.')\n",
      "(0.009136178522364903, 'He used his first break point to close out the first set before going up 3-0 in the second and wrapping up the win on his first match point.')\n",
      "(0.009104756512964777, \"The Spaniard broke Anderson twice in the second but didn't get another chance on the South African's serve in the final set.\")\n",
      "(0.00910262574670152, '\"We also had the impression that at this stage it might be better to play matches than to train.')\n",
      "(0.009097421351821915, 'The competition is set to feature 18 countries in the November 18-24 finals in Madrid next year, and will replace the classic home-and-away ties played four times per year for decades.')\n",
      "(0.009094807375780006, 'Federer said earlier this month in Shanghai in that his chances of playing the Davis Cup were all but non-existent.')\n"
     ]
    }
   ],
   "source": [
    "# Extract top 10 sentences as the summary\n",
    "for i in range(10):\n",
    "  print(ranked_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0012695751770095795, \"'No, not at all.\")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_sentences[118]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
